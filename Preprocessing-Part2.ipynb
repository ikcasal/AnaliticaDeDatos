{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING (Part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a powerful package written in Python and developed by part of the developers of Scikit-Learn, called Imbalanced-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is developed through GitHub (see https://github.com/scikit-learn-contrib/imbalanced-learn), and there is also an official website (see http://imbalanced-learn.org/en/stable/) where you can find all the info you might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I strongly recommend to read the user guide (see http://imbalanced-learn.org/en/stable/user_guide.html) as well as the general examples as a complement to it (see http://imbalanced-learn.org/en/stable/auto_examples/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is not available through Anaconda Navigator, but you can install install is from the prompt by entering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try NearMiss undersampling technique on Iris dataset. Since Iris is perfectly balanced, firstly we will imbalance it artificially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# Load dataset and create an artificial imbalance\n",
    "iris = load_iris()\n",
    "X, y = make_imbalance(iris.data, iris.target,\n",
    "                      sampling_strategy={0: 25, 1: 50, 2: 50},\n",
    "                      random_state=RANDOM_STATE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Training statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Creation of a pipeline, i.e. concatenation of steps in a composed process (see documentation for further details)\n",
    "pipeline = make_pipeline(NearMiss(version=2),\n",
    "                         LinearSVC(random_state=RANDOM_STATE, max_iter=10000))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try now SMOTE oversampling technique on a dataset about thyroid sickness. It has 3772 samples and 52 independent variables. It is imbalanced by a rate of 15 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "tiroides = pd.read_csv('Thyroids.csv')\n",
    "tiroides.values.astype(float)\n",
    "\n",
    "# Separate inputs and target\n",
    "X = tiroides.values[:,:-1]\n",
    "y = tiroides.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Training statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline = make_pipeline(SMOTE(random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=1000, random_state=RANDOM_STATE))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(classification_report_imbalanced(y_test, y_pred))\n",
    "print('AUC = ' + str(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "(i) Try a different NearMiss version from the one in the example for the thyroids dataset with random forests classifier. Does it get better if we increase the number of trees to 100 in the forest (n_estimators)? And from 100 to 1000?\n",
    "\n",
    "(ii) Plan a mixed strategy for thyroids dataset and chech its performance with random forests. Play with n_estimators parameter to increase f1 average score. Is the order of the mixed sampling strategies relevant?\n",
    "\n",
    "(iii) Combine PCA with the mixed strategy. Quantify the percentage of data compression and get the number of PCs used, when capturing 95% and 99% of the total cummulative variance. Compare the performance with the one in (ii). In case of big differencies, which could be the reason?\n",
    "Try to specify the number of PCs, instead of the percentage of variance, by chosing n_components to be an integer, e.g. n_components=15. Play with the number to try to get a good solution with the maximum possible compression. Do you want to change/qualify your answer to the previous question?\n",
    "\n",
    "(iv) Compare the results of all strategies with the case of not correcting the imbalance. Is it beneficial to correct the lack of balance?\n",
    "\n",
    "(v) Use ADASYN oversampling technique combined with an undersampling technique different from NearMiss. Explain the reason for your choice. See imbalanced-learn documentation for seeing which functions to use and checking how to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "(i) Try a different NearMiss version from the one in the example for the thyroids dataset with random forests classifier. Does it get better if we increase the number of trees to 100 in the forest (n_estimators)? And from 100 to 1000?\n",
    "\n",
    "(ii) Plan a mixed strategy for thyroids dataset and chech its performance with random forests. Play with n_estimators parameter to increase f1 average score. Is the order of the mixed sampling strategies relevant?\n",
    "\n",
    "(iii) Combine PCA with the mixed strategy. Quantify the percentage of data compression and get the number of PCs used, when capturing 95% and 99% of the total cummulative variance. Compare the performance with the one in (ii). In case of big differencies, which could be the reason?\n",
    "Try to specify the number of PCs, instead of the percentage of variance, by chosing n_components to be an integer, e.g. n_components=15. Play with the number to try to get a good solution with the maximum possible compression. Do you want to change/qualify your answer to the previous question?\n",
    "\n",
    "(iv) Compare the results of all strategies with the case of not correcting the imbalance. Is it beneficial to correct the lack of balance?\n",
    "\n",
    "(v) Use ADASYN oversampling technique combined with an undersampling technique different from NearMiss. Explain the reason for your choice. See imbalanced-learn documentation for seeing which functions to use and checking how to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# In general, and for your future revisions of the material, it is better that you provided a complete code here.\n",
    "# So it is better to define imports and functions here, so that this one single cell could be executed on its own.\n",
    "\n",
    "# (i)\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomizing seed\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# Load dataset\n",
    "tiroides = pd.read_csv('Thyroids.csv')\n",
    "tiroides.values.astype(float)\n",
    "\n",
    "# Separate inputs and target\n",
    "X = tiroides.values[:,:-1]\n",
    "y = tiroides.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "# Pipelines creation\n",
    "pipeline10 = make_pipeline(NearMiss(version=3, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=10, random_state=RANDOM_STATE))\n",
    "pipeline10.fit(X_train, y_train)\n",
    "\n",
    "pipeline100 = make_pipeline(NearMiss(version=3, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "pipeline100.fit(X_train, y_train)\n",
    "\n",
    "pipeline1000 = make_pipeline(NearMiss(version=3, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=1000, random_state=RANDOM_STATE))\n",
    "pipeline1000.fit(X_train, y_train)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(\"Results for n_estimators = 10\")\n",
    "print(classification_report_imbalanced(y_test, pipeline10.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))\n",
    "\n",
    "print(\"Results for n_estimators = 100\")\n",
    "print(classification_report_imbalanced(y_test, pipeline100.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))\n",
    "\n",
    "print(\"Results for n_estimators = 1000\")\n",
    "print(classification_report_imbalanced(y_test, pipeline1000.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of trees to 100, makes the behavior much better. Nevertheless, the improvement from 100 to 1000 y small and the difference in computational time is big (10 times bigger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ii)\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Randomizing seed\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# Load dataset\n",
    "tiroides = pd.read_csv('Tyroids.csv')\n",
    "tiroides.values.astype(float)\n",
    "\n",
    "# Separate inputs and target\n",
    "X = tiroides.values[:,:-1]\n",
    "y = tiroides.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "# Pipelines creation\n",
    "pipeline_under_over_10 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                                    SMOTE(random_state=RANDOM_STATE),\n",
    "                                    RandomForestClassifier(n_estimators=10, random_state=RANDOM_STATE))\n",
    "pipeline_under_over_10.fit(X_train, y_train)\n",
    "\n",
    "pipeline_under_over_100 = make_pipeline(NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                                    SMOTE(random_state=RANDOM_STATE),\n",
    "                                    RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "pipeline_under_over_100.fit(X_train, y_train)\n",
    "\n",
    "pipeline_over_under_10 = make_pipeline(SMOTE(random_state=RANDOM_STATE),\n",
    "                         NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=10, random_state=RANDOM_STATE))\n",
    "pipeline_over_under_10.fit(X_train, y_train)\n",
    "\n",
    "pipeline_over_under_100 = make_pipeline(SMOTE(random_state=RANDOM_STATE),\n",
    "                         NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "pipeline_over_under_100.fit(X_train, y_train)\n",
    "\n",
    "# Classification and results presentation\n",
    "print(\"Results for first under- then oversampling\")\n",
    "print(\"Results for n_estimators = 10\")\n",
    "print(classification_report_imbalanced(y_test, pipeline_under_over_10.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))\n",
    "print(\"Results for n_estimators = 100\")\n",
    "print(classification_report_imbalanced(y_test, pipeline_under_over_100.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))\n",
    "\n",
    "print(\"Results for first over- then undersampling\")\n",
    "print(\"Results for n_estimators = 10\")\n",
    "print(classification_report_imbalanced(y_test, pipeline_over_under_10.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))\n",
    "print(\"Results for n_estimators = 100\")\n",
    "print(classification_report_imbalanced(y_test, pipeline_over_under_100.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the strategy matters, being clearly necessary to perform first over- and then undersampling. The benefit for using 100 trees instead of 10 in that case is not big, but in both cases the computation time is short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (iii)\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def pca_projections(df, n_components=0.95):\n",
    "    pca = PCA(n_components)\n",
    "    X = df[df.columns[:-1]]  # Assuming the class in in the last column\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "    proj_df = pd.DataFrame(data=X, columns=['PC' + str(x) for x in list(range(1, X.shape[1] + 1))])\n",
    "    proj_df = pd.concat([proj_df, df[df.columns[-1]]], axis=1)\n",
    "    return proj_df\n",
    "\n",
    "\n",
    "# Randomizing seed\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# Load dataset\n",
    "tiroides = pd.read_csv('Tyroids.csv')\n",
    "tiroides.values.astype(float)\n",
    "\n",
    "# Data compression by PCA\n",
    "n_components = 0.999999\n",
    "tiroides = pca_projections(tiroides, n_components)\n",
    "if n_components < 1.0:\n",
    "    n_comp = pca_projections(tiroides, n_components).shape[1] - 1\n",
    "else:\n",
    "    n_comp = n_components\n",
    "\n",
    "# Separate inputs and target\n",
    "X = tiroides.values[:,:-1]\n",
    "y = tiroides.values[:,-1].astype(int)\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "# Pipeline creation\n",
    "pipeline_pca_over_under_100 = make_pipeline(SMOTE(random_state=RANDOM_STATE),\n",
    "                         NearMiss(version=2, random_state=RANDOM_STATE),\n",
    "                         RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "pipeline_pca_over_under_100.fit(X_train, y_train)\n",
    "\n",
    "print(\"Results for pca projected data, using \" + str(n_comp) + \" PCs , and for first over- then undersampling with n_estimators = 100\")\n",
    "print(classification_report_imbalanced(y_test, pipeline_pca_over_under_100.predict(X_test), digits=4, target_names=[\"Healthy\", \"Sick\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 95% and 99%, also 99.9%, the amount of PCs is 4, with poor performance. Raising it up to 99.99%, it goes up to 11 PCs, with an acceptable performance that is comparable to the previous ones but a bit worse. Next jump, comes at a level of 99.999%, with 24 PCs but almost the same result as with 11 PCs. No further improvement is possible, even taking the number of PCs close to the maximum (52, that is the total amount of independent variables).\n",
    "\n",
    "The reason could be that by projecting we loose information that is non-linear, even when we capture almost 100% of the linear variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv)\n",
    "\n",
    "In general, the correction is slightly beneficial even when we have not used all the potential of the sampling algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
